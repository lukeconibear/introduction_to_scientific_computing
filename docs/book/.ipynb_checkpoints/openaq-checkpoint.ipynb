{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "express-bearing",
   "metadata": {},
   "source": [
    "# OpenAQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-planner",
   "metadata": {},
   "source": [
    "Download OpenAQ data and convert to Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-california",
   "metadata": {},
   "source": [
    "## 1. Download the data\n",
    "\n",
    "Using `download.py` as below, data in `.ndjson.gz` format for a given date range (e.g. 1st Jan 2015 to 31st Dec 2015) using:\n",
    "\n",
    "```\n",
    "python download.py 2015-01-01 2015-12-31\n",
    "```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Download script for all OpenAQ data\n",
    "Credit to https://github.com/barronh/scrapenaq\n",
    "Downloads all (.ndjson.gz) files between dates\n",
    "Example:\n",
    "    python download.py 2020-01-01 2020-12-31\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import re\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('startdate', help='Find ndjson.gz created >= startdate')\n",
    "parser.add_argument('enddate', help='Find ndjson.gz created <= enddate')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "dates = pd.date_range(args.startdate, args.enddate)\n",
    "keyre = re.compile('<Key>(.+?)</Key>')\n",
    "BROOT = 'openaq-fetches.s3.amazonaws.com/'\n",
    "\n",
    "for date in dates:\n",
    "    xrpath = (\n",
    "        'https://{}?delimiter=%2F&'.format(BROOT) +\n",
    "        'prefix=realtime-gzipped%2F{}%2F'.format(date.strftime('%F'))\n",
    "    )\n",
    "    xmlpath = BROOT + date.strftime('realtime-gzipped/%F.xml')\n",
    "    zippedpath = BROOT + date.strftime('realtime-gzipped/%F')\n",
    "    \n",
    "    os.makedirs(zippedpath, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(xmlpath):\n",
    "        print('Keeping cached', xmlpath)\n",
    "    else:\n",
    "        urllib.request.urlretrieve(xrpath, xmlpath)\n",
    "\n",
    "    xmltxt = open(xmlpath, mode='r').read()\n",
    "    keys = keyre.findall(xmltxt)\n",
    "    \n",
    "    for key in keys:\n",
    "        url = 'https://' + BROOT + key\n",
    "        outpath = BROOT + key\n",
    "        if os.path.exists(outpath):\n",
    "            print('Keeping cached', outpath)\n",
    "        else:\n",
    "            urllib.request.urlretrieve(url, outpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-shannon",
   "metadata": {},
   "source": [
    "## 2. Convert to a Pandas DataFrame\n",
    "Using `convert.py` below, convert the `.ndjson.gz` data to a Pandas DataFrame for a given year (e.g. 2015) using:  \n",
    "```\n",
    "python convert.py 2015\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-willow",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Convert the default format for OpenAQ data (.ndjson.gz) to Pandas DataFrame for a given year\n",
    "\"\"\"\n",
    "\n",
    "import glob\n",
    "import gzip\n",
    "import ndjson\n",
    "from pandas.io.json import json_normalize\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('year')\n",
    "args = parser.parse_args()\n",
    "\n",
    "year = args.year\n",
    "\n",
    "path = '~' # change to this where the data is\n",
    "files = sorted(glob.glob('openaq-fetches.s3.amazonaws.com/realtime-gzipped/' + year + '*/*'))\n",
    "\n",
    "df_list = []\n",
    "for file in files:\n",
    "    with gzip.open(file, 'rb') as ds:\n",
    "        data_ndjson = ds.read()\n",
    "\n",
    "    data_json = ndjson.loads(data_ndjson)\n",
    "\n",
    "    df_list.append(json_normalize(data_json))\n",
    "    \n",
    "df = pd.concat(df_list, sort=False)\n",
    "\n",
    "df.set_index('date.utc', inplace=True)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "df.to_csv(path + 'openaq_data_' + year + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-camcorder",
   "metadata": {},
   "source": [
    "## To analyse the OpenAQ data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-activity",
   "metadata": {},
   "source": [
    "### Option 1: eagerly load into memory using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-ontario",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    '/nfs/b0004/Users/earlacoa/openaq/shared/openaq_data_2013-2020_india.csv', \n",
    "    parse_dates=['date.utc'],\n",
    "    usecols=['date.utc', 'parameter', 'value', 'unit', 'coordinates.latitude', 'coordinates.longitude', 'city', 'country'],\n",
    "    index_col='date.utc'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-shipping",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['2020-02-01':'2020-04-30']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-deviation",
   "metadata": {},
   "source": [
    "### Option 2: lazily load using Dask with parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-commerce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet(\n",
    "    '/nfs/b0004/Users/earlacoa/openaq/shared/openaq_data_2013-2020_india.parquet',\n",
    "    columns=['parameter', 'value', 'unit', 'coordinates.latitude', 'coordinates.longitude', 'city', 'country']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-miami",
   "metadata": {},
   "source": [
    "To compute tasks use the `.compute()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-grade",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-orange",
   "metadata": {},
   "source": [
    "## Convert a dataset to the OpenAQ format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_openaq = pd.read_csv(\n",
    "    '/nfs/b0122/Users/earlacoa/openaq/csv/openaq_data_2015_noduplicates.csv',\n",
    "    index_col=\"date.utc\",\n",
    "    parse_dates=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obs_summaries = {'2014': [], '2015': [], '2016': [], '2017': [], '2018': [], '2019': [], '2020': []}\n",
    "china_obs_files = glob.glob('/nfs/a68/earlacoa/china_measurements_corrected/*nc')\n",
    "parameters = {'CO': 'co', 'NO2': 'no2', 'O3': 'o3', 'PM10': 'pm10', 'PM2.5': 'pm25', 'SO2': 'so2'}\n",
    "years = ['2014', '2015', '2016', '2017', '2018', '2019', '2020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-fisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "for china_obs_file in china_obs_files:\n",
    "    ds_obs = xr.open_dataset(china_obs_file)\n",
    "    \n",
    "    for parameter in parameters.keys():\n",
    "        dict_obs = {\n",
    "            'date.utc': ds_obs.time.values,\n",
    "            'city': ds_obs.city,\n",
    "            'unit': 'µg/m³',\n",
    "            'value': ds_obs[parameter].values,\n",
    "            'country': 'CN',\n",
    "            'location': ds_obs.name,\n",
    "            'parameter': parameters[parameter],\n",
    "            'sourceName': 'China measurements',\n",
    "            'sourceType': 'government',\n",
    "            'date.local': ds_obs.time.values + np.timedelta64(8, 'h'),\n",
    "            'coordinates.latitude': ds_obs.lat,\n",
    "            'coordinates.longitude': ds_obs.lon, \n",
    "            'averagingPeriod.unit': 'hours',\n",
    "            'averagingPeriod.value': 1\n",
    "        }\n",
    "        df_obs = pd.DataFrame.from_dict(dict_obs)\n",
    "        df_obs.set_index('date.utc', inplace=True)\n",
    "        \n",
    "        for year in years:\n",
    "            df_obs_summaries[year].append(df_obs[year])\n",
    "        \n",
    "    ds_obs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    df_obs_summaries_concat = pd.concat(df_obs_summaries[year])\n",
    "    df_obs_summaries_concat.to_csv(f'/nfs/a68/earlacoa/china_measurements_corrected/df_obs_summary_{year}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
